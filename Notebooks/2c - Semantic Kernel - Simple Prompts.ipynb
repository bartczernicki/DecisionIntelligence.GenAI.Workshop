{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;padding-right: 10px\" width =\"40px\" src=\"https://raw.githubusercontent.com/bartczernicki/DecisionIntelligence.GenAI.Workshop/main/Images/SemanticKernelLogo.png\">\n",
    "\n",
    "## Semantic Kernel - Simple Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Intelligence applied in this module:  \n",
    "* Listing of various decision-making frameworks and with their descriptions by using GenAI prompt engineering   \n",
    "\n",
    "Prompts are the fundemental building block to getting the proper responses from AI models. This module demonstrates how to use common prompt engineering techniques while using Semantic Kernel. If you've ever used ChatGPT or Microsoft Copilot, you're already familiar with prompting. Given a prompt instruction, an LLM will attempt to predict the most likely response. With Semantic Kernel, you can craft applications, services & APIs that execute prompts.  \n",
    "\n",
    "For more information on using prompts with Semantic Kernel, visit: https://learn.microsoft.com/en-us/semantic-kernel/prompts/your-first-prompt  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Initialize Configuration Builder & Build the Semantic Kernel Orchestration\n",
    "\n",
    "Execute the next two cells to:\n",
    "* Use the Configuration Builder to load the API secrets.  \n",
    "* Use the API configuration to build the Semantic Kernel orchestrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Microsoft.Extensions.Configuration, 8.0.0</span></li><li><span>Microsoft.Extensions.Configuration.Json, 8.0.0</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Import the required NuGet configuration packages\n",
    "#r \"nuget: Microsoft.Extensions.Configuration, 8.0.0\"\n",
    "#r \"nuget: Microsoft.Extensions.Configuration.Json, 8.0.0\"\n",
    "\n",
    "using Microsoft.Extensions.Configuration;\n",
    "using System.IO;\n",
    "\n",
    "// Load the configuration settings from the local.settings.json and secrets.settings.json files\n",
    "// The secrets.settings.json file is used to store sensitive information such as API keys\n",
    "var configurationBuilder = new ConfigurationBuilder()\n",
    "    .SetBasePath(Directory.GetCurrentDirectory())\n",
    "    .AddJsonFile(\"local.settings.json\", optional: true, reloadOnChange: true)\n",
    "    .AddJsonFile(\"secrets.settings.json\", optional: true, reloadOnChange: true);\n",
    "var config = configurationBuilder.Build();\n",
    "\n",
    "// IMPORTANT: You ONLY NEED either Azure OpenAI or OpenAI connectiopn info, not both.\n",
    "// Azure OpenAI Connection Info\n",
    "var azureOpenAIEndpoint = config[\"AzureOpenAI:Endpoint\"];\n",
    "var azureOpenAIAPIKey = config[\"AzureOpenAI:APIKey\"];\n",
    "var azureOpenAIModelDeploymentName = config[\"AzureOpenAI:ModelDeploymentName\"];\n",
    "// OpenAI Connection Info \n",
    "var openAIAPIKey = config[\"OpenAI:APIKey\"];\n",
    "var openAIModelId = config[\"OpenAI:ModelId\"];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Microsoft.SemanticKernel, 1.20.0</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Azure OpenAI Service\r\n"
     ]
    }
   ],
   "source": [
    "// Import the Semantic Kernel NuGet package\n",
    "#r \"nuget: Microsoft.SemanticKernel, 1.20.0\"\n",
    "\n",
    "using Microsoft.SemanticKernel;\n",
    "using Microsoft.SemanticKernel.ChatCompletion;\n",
    "using Microsoft.SemanticKernel.Connectors.OpenAI;\n",
    "\n",
    "Kernel semanticKernel;\n",
    "\n",
    "// Set the flag to use Azure OpenAI or OpenAI. False to use OpenAI, True to use Azure OpenAI\n",
    "var useAzureOpenAI = true;\n",
    "\n",
    "// Create a new Semantic Kernel instance\n",
    "if (useAzureOpenAI)\n",
    "{\n",
    "    Console.WriteLine(\"Using Azure OpenAI Service\");\n",
    "    semanticKernel = Kernel.CreateBuilder()\n",
    "        .AddAzureOpenAIChatCompletion(\n",
    "            deploymentName: azureOpenAIModelDeploymentName,\n",
    "            endpoint: azureOpenAIEndpoint,\n",
    "            apiKey: azureOpenAIAPIKey)\n",
    "        .Build();\n",
    "}\n",
    "else\n",
    "{\n",
    "    Console.WriteLine(\"Using OpenAI Service\");\n",
    "    semanticKernel = Kernel.CreateBuilder()\n",
    "        .AddOpenAIChatCompletion(\n",
    "            modelId: openAIModelId,\n",
    "            apiKey: openAIAPIKey)\n",
    "        .Build();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Execute a Semantic Kernel Prompt\n",
    "\n",
    "Many LLMs and even SLMs have been trained on knowledge that includes decision frameworks & processes. This makes LLMs great assistants to:\n",
    "* Provide proper decision frames\n",
    "* Gather the information & intelligence in order to make a decision\n",
    "* Recommend decision frameworks to make a high-quality decision\n",
    "* Provide feedback to evaluate a decision\n",
    "\n",
    "Once the Semantic Kernel instance is built, it is ready to intake prompt instructions. In the prompt below, the Semantic Kernel is instructed to provide examples of decision frameworks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Microsoft.SemanticKernel.HttpOperationException: No such host is known. (yourendpoint.openai.azure.com:443)\r\n ---> System.ClientModel.ClientResultException: No such host is known. (yourendpoint.openai.azure.com:443)\r\n ---> System.Net.Http.HttpRequestException: No such host is known. (yourendpoint.openai.azure.com:443)\r\n ---> System.Net.Sockets.SocketException (11001): No such host is known.\r\n   at System.Net.Sockets.Socket.AwaitableSocketAsyncEventArgs.ThrowException(SocketError error, CancellationToken cancellationToken)\r\n   at System.Net.Sockets.Socket.AwaitableSocketAsyncEventArgs.System.Threading.Tasks.Sources.IValueTaskSource.GetResult(Int16 token)\r\n   at System.Net.Sockets.Socket.<ConnectAsync>g__WaitForConnectWithCancellation|285_0(AwaitableSocketAsyncEventArgs saea, ValueTask connectTask, CancellationToken cancellationToken)\r\n   at System.Net.Http.HttpConnectionPool.ConnectToTcpHostAsync(String host, Int32 port, HttpRequestMessage initialRequest, Boolean async, CancellationToken cancellationToken)\r\n   --- End of inner exception stack trace ---\r\n   at System.Net.Http.HttpConnectionPool.ConnectToTcpHostAsync(String host, Int32 port, HttpRequestMessage initialRequest, Boolean async, CancellationToken cancellationToken)\r\n   at System.Net.Http.HttpConnectionPool.ConnectAsync(HttpRequestMessage request, Boolean async, CancellationToken cancellationToken)\r\n   at System.Net.Http.HttpConnectionPool.CreateHttp11ConnectionAsync(HttpRequestMessage request, Boolean async, CancellationToken cancellationToken)\r\n   at System.Net.Http.HttpConnectionPool.AddHttp11ConnectionAsync(QueueItem queueItem)\r\n   at System.Threading.Tasks.TaskCompletionSourceWithCancellation`1.WaitWithCancellationAsync(CancellationToken cancellationToken)\r\n   at System.Net.Http.HttpConnectionPool.SendWithVersionDetectionAndRetryAsync(HttpRequestMessage request, Boolean async, Boolean doRequestAuth, CancellationToken cancellationToken)\r\n   at System.Net.Http.DiagnosticsHandler.SendAsyncCore(HttpRequestMessage request, Boolean async, CancellationToken cancellationToken)\r\n   at System.Net.Http.RedirectHandler.SendAsync(HttpRequestMessage request, Boolean async, CancellationToken cancellationToken)\r\n   at System.Net.Http.HttpClient.<SendAsync>g__Core|83_0(HttpRequestMessage request, HttpCompletionOption completionOption, CancellationTokenSource cts, Boolean disposeCts, CancellationTokenSource pendingRequestsCts, CancellationToken originalCancellationToken)\r\n   at System.ClientModel.Primitives.HttpClientPipelineTransport.ProcessSyncOrAsync(PipelineMessage message, Boolean async)\r\n   --- End of inner exception stack trace ---\r\n   at System.ClientModel.Primitives.HttpClientPipelineTransport.ProcessSyncOrAsync(PipelineMessage message, Boolean async)\r\n   at System.ClientModel.Primitives.HttpClientPipelineTransport.ProcessCoreAsync(PipelineMessage message)\r\n   at System.ClientModel.Primitives.PipelineTransport.ProcessSyncOrAsync(PipelineMessage message, Boolean async)\r\n   at System.ClientModel.Primitives.PipelineTransport.ProcessAsync(PipelineMessage message)\r\n   at System.ClientModel.Primitives.PipelineTransport.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n   at System.ClientModel.Primitives.PipelinePolicy.ProcessNextAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n   at System.ClientModel.Primitives.ApiKeyAuthenticationPolicy.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n   at System.ClientModel.Primitives.PipelinePolicy.ProcessNextAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n   at System.ClientModel.Primitives.ClientRetryPolicy.ProcessSyncOrAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex, Boolean async)\r\n   at System.ClientModel.Primitives.ClientRetryPolicy.ProcessSyncOrAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex, Boolean async)\r\n   at System.ClientModel.Primitives.ClientRetryPolicy.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n   at GenericActionPipelinePolicy.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n   at System.ClientModel.Primitives.PipelinePolicy.ProcessNextAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n   at Azure.AI.OpenAI.GenericActionPipelinePolicy.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n   at System.ClientModel.Primitives.PipelinePolicy.ProcessNextAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n   at Azure.AI.OpenAI.GenericActionPipelinePolicy.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n   at System.ClientModel.Primitives.ClientPipeline.SendAsync(PipelineMessage message)\r\n   at Azure.AI.OpenAI.ClientPipelineExtensions.ProcessMessageAsync(ClientPipeline pipeline, PipelineMessage message, RequestOptions options)\r\n   at Azure.AI.OpenAI.Chat.AzureChatClient.CompleteChatAsync(BinaryContent content, RequestOptions options)\r\n   at OpenAI.Chat.ChatClient.CompleteChatAsync(IEnumerable`1 messages, ChatCompletionOptions options, CancellationToken cancellationToken)\r\n   at Microsoft.SemanticKernel.Connectors.OpenAI.ClientCore.RunRequestAsync[T](Func`1 request)\r\n   --- End of inner exception stack trace ---\r\n   at Microsoft.SemanticKernel.Connectors.OpenAI.ClientCore.RunRequestAsync[T](Func`1 request)\r\n   at Microsoft.SemanticKernel.Connectors.OpenAI.ClientCore.GetChatMessageContentsAsync(String targetModel, ChatHistory chatHistory, PromptExecutionSettings executionSettings, Kernel kernel, CancellationToken cancellationToken)\r\n   at Microsoft.SemanticKernel.KernelFunctionFromPrompt.GetChatCompletionResultAsync(IChatCompletionService chatCompletion, Kernel kernel, PromptRenderingResult promptRenderingResult, CancellationToken cancellationToken)\r\n   at Microsoft.SemanticKernel.KernelFunctionFromPrompt.InvokeCoreAsync(Kernel kernel, KernelArguments arguments, CancellationToken cancellationToken)\r\n   at Microsoft.SemanticKernel.KernelFunction.<>c__DisplayClass21_0.<<InvokeAsync>b__0>d.MoveNext()\r\n--- End of stack trace from previous location ---\r\n   at Microsoft.SemanticKernel.Kernel.InvokeFilterOrFunctionAsync(NonNullCollection`1 functionFilters, Func`2 functionCallback, FunctionInvocationContext context, Int32 index)\r\n   at Microsoft.SemanticKernel.Kernel.OnFunctionInvocationAsync(KernelFunction function, KernelArguments arguments, FunctionResult functionResult, Func`2 functionCallback, CancellationToken cancellationToken)\r\n   at Microsoft.SemanticKernel.KernelFunction.InvokeAsync(Kernel kernel, KernelArguments arguments, CancellationToken cancellationToken)\r\n   at Submission#5.<<Initialize>>d__0.MoveNext()\r\n--- End of stack trace from previous location ---\r\n   at Microsoft.CodeAnalysis.Scripting.ScriptExecutionState.RunSubmissionsAsync[TResult](ImmutableArray`1 precedingExecutors, Func`2 currentExecutor, StrongBox`1 exceptionHolderOpt, Func`2 catchExceptionOpt, CancellationToken cancellationToken)",
     "output_type": "error",
     "traceback": [
      "Microsoft.SemanticKernel.HttpOperationException: No such host is known. (yourendpoint.openai.azure.com:443)\r\n",
      " ---> System.ClientModel.ClientResultException: No such host is known. (yourendpoint.openai.azure.com:443)\r\n",
      " ---> System.Net.Http.HttpRequestException: No such host is known. (yourendpoint.openai.azure.com:443)\r\n",
      " ---> System.Net.Sockets.SocketException (11001): No such host is known.\r\n",
      "   at System.Net.Sockets.Socket.AwaitableSocketAsyncEventArgs.ThrowException(SocketError error, CancellationToken cancellationToken)\r\n",
      "   at System.Net.Sockets.Socket.AwaitableSocketAsyncEventArgs.System.Threading.Tasks.Sources.IValueTaskSource.GetResult(Int16 token)\r\n",
      "   at System.Net.Sockets.Socket.<ConnectAsync>g__WaitForConnectWithCancellation|285_0(AwaitableSocketAsyncEventArgs saea, ValueTask connectTask, CancellationToken cancellationToken)\r\n",
      "   at System.Net.Http.HttpConnectionPool.ConnectToTcpHostAsync(String host, Int32 port, HttpRequestMessage initialRequest, Boolean async, CancellationToken cancellationToken)\r\n",
      "   --- End of inner exception stack trace ---\r\n",
      "   at System.Net.Http.HttpConnectionPool.ConnectToTcpHostAsync(String host, Int32 port, HttpRequestMessage initialRequest, Boolean async, CancellationToken cancellationToken)\r\n",
      "   at System.Net.Http.HttpConnectionPool.ConnectAsync(HttpRequestMessage request, Boolean async, CancellationToken cancellationToken)\r\n",
      "   at System.Net.Http.HttpConnectionPool.CreateHttp11ConnectionAsync(HttpRequestMessage request, Boolean async, CancellationToken cancellationToken)\r\n",
      "   at System.Net.Http.HttpConnectionPool.AddHttp11ConnectionAsync(QueueItem queueItem)\r\n",
      "   at System.Threading.Tasks.TaskCompletionSourceWithCancellation`1.WaitWithCancellationAsync(CancellationToken cancellationToken)\r\n",
      "   at System.Net.Http.HttpConnectionPool.SendWithVersionDetectionAndRetryAsync(HttpRequestMessage request, Boolean async, Boolean doRequestAuth, CancellationToken cancellationToken)\r\n",
      "   at System.Net.Http.DiagnosticsHandler.SendAsyncCore(HttpRequestMessage request, Boolean async, CancellationToken cancellationToken)\r\n",
      "   at System.Net.Http.RedirectHandler.SendAsync(HttpRequestMessage request, Boolean async, CancellationToken cancellationToken)\r\n",
      "   at System.Net.Http.HttpClient.<SendAsync>g__Core|83_0(HttpRequestMessage request, HttpCompletionOption completionOption, CancellationTokenSource cts, Boolean disposeCts, CancellationTokenSource pendingRequestsCts, CancellationToken originalCancellationToken)\r\n",
      "   at System.ClientModel.Primitives.HttpClientPipelineTransport.ProcessSyncOrAsync(PipelineMessage message, Boolean async)\r\n",
      "   --- End of inner exception stack trace ---\r\n",
      "   at System.ClientModel.Primitives.HttpClientPipelineTransport.ProcessSyncOrAsync(PipelineMessage message, Boolean async)\r\n",
      "   at System.ClientModel.Primitives.HttpClientPipelineTransport.ProcessCoreAsync(PipelineMessage message)\r\n",
      "   at System.ClientModel.Primitives.PipelineTransport.ProcessSyncOrAsync(PipelineMessage message, Boolean async)\r\n",
      "   at System.ClientModel.Primitives.PipelineTransport.ProcessAsync(PipelineMessage message)\r\n",
      "   at System.ClientModel.Primitives.PipelineTransport.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n",
      "   at System.ClientModel.Primitives.PipelinePolicy.ProcessNextAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n",
      "   at System.ClientModel.Primitives.ApiKeyAuthenticationPolicy.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n",
      "   at System.ClientModel.Primitives.PipelinePolicy.ProcessNextAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n",
      "   at System.ClientModel.Primitives.ClientRetryPolicy.ProcessSyncOrAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex, Boolean async)\r\n",
      "   at System.ClientModel.Primitives.ClientRetryPolicy.ProcessSyncOrAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex, Boolean async)\r\n",
      "   at System.ClientModel.Primitives.ClientRetryPolicy.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n",
      "   at GenericActionPipelinePolicy.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n",
      "   at System.ClientModel.Primitives.PipelinePolicy.ProcessNextAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n",
      "   at Azure.AI.OpenAI.GenericActionPipelinePolicy.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n",
      "   at System.ClientModel.Primitives.PipelinePolicy.ProcessNextAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n",
      "   at Azure.AI.OpenAI.GenericActionPipelinePolicy.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n",
      "   at System.ClientModel.Primitives.ClientPipeline.SendAsync(PipelineMessage message)\r\n",
      "   at Azure.AI.OpenAI.ClientPipelineExtensions.ProcessMessageAsync(ClientPipeline pipeline, PipelineMessage message, RequestOptions options)\r\n",
      "   at Azure.AI.OpenAI.Chat.AzureChatClient.CompleteChatAsync(BinaryContent content, RequestOptions options)\r\n",
      "   at OpenAI.Chat.ChatClient.CompleteChatAsync(IEnumerable`1 messages, ChatCompletionOptions options, CancellationToken cancellationToken)\r\n",
      "   at Microsoft.SemanticKernel.Connectors.OpenAI.ClientCore.RunRequestAsync[T](Func`1 request)\r\n",
      "   --- End of inner exception stack trace ---\r\n",
      "   at Microsoft.SemanticKernel.Connectors.OpenAI.ClientCore.RunRequestAsync[T](Func`1 request)\r\n",
      "   at Microsoft.SemanticKernel.Connectors.OpenAI.ClientCore.GetChatMessageContentsAsync(String targetModel, ChatHistory chatHistory, PromptExecutionSettings executionSettings, Kernel kernel, CancellationToken cancellationToken)\r\n",
      "   at Microsoft.SemanticKernel.KernelFunctionFromPrompt.GetChatCompletionResultAsync(IChatCompletionService chatCompletion, Kernel kernel, PromptRenderingResult promptRenderingResult, CancellationToken cancellationToken)\r\n",
      "   at Microsoft.SemanticKernel.KernelFunctionFromPrompt.InvokeCoreAsync(Kernel kernel, KernelArguments arguments, CancellationToken cancellationToken)\r\n",
      "   at Microsoft.SemanticKernel.KernelFunction.<>c__DisplayClass21_0.<<InvokeAsync>b__0>d.MoveNext()\r\n",
      "--- End of stack trace from previous location ---\r\n",
      "   at Microsoft.SemanticKernel.Kernel.InvokeFilterOrFunctionAsync(NonNullCollection`1 functionFilters, Func`2 functionCallback, FunctionInvocationContext context, Int32 index)\r\n",
      "   at Microsoft.SemanticKernel.Kernel.OnFunctionInvocationAsync(KernelFunction function, KernelArguments arguments, FunctionResult functionResult, Func`2 functionCallback, CancellationToken cancellationToken)\r\n",
      "   at Microsoft.SemanticKernel.KernelFunction.InvokeAsync(Kernel kernel, KernelArguments arguments, CancellationToken cancellationToken)\r\n",
      "   at Submission#5.<<Initialize>>d__0.MoveNext()\r\n",
      "--- End of stack trace from previous location ---\r\n",
      "   at Microsoft.CodeAnalysis.Scripting.ScriptExecutionState.RunSubmissionsAsync[TResult](ImmutableArray`1 precedingExecutors, Func`2 currentExecutor, StrongBox`1 exceptionHolderOpt, Func`2 catchExceptionOpt, CancellationToken cancellationToken)"
     ]
    }
   ],
   "source": [
    "// New prompt to help with decision-making frameworks\n",
    "var simplePrompt = \"\"\"\n",
    "Identify and list various decision-making frameworks that can enhance the quality of decisions. \n",
    "Briefly describe how each framework supports better analysis and reasoning in different decision-making scenarios.\n",
    "\"\"\";\n",
    "\n",
    "var simplePromptResponse = await semanticKernel.InvokePromptAsync(simplePrompt);\n",
    "Console.WriteLine(simplePromptResponse.GetValue<string>());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Execute a Semantic Kernel Prompt with Streaming\n",
    "\n",
    "Semantic Kernel supports prompt response streaming when invoking the prompt. This allows responses to be streamed to the client as soon as they are made available by the LLM and service. Below the same decision prompt is executed. However, notice that chunks are streamed and can be read by the user as soon as they appear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here are several decision-making frameworks that can enhance the quality of decisions, along with brief descriptions of how each supports better analysis and reasoning:\n",
      "\n",
      "1. **SWOT Analysis (Strengths, Weaknesses, Opportunities, Threats)**:\n",
      "   - **Description**: This framework helps individuals and organizations assess both internal and external factors that can affect decision-making. \n",
      "   - **Support**: By identifying strengths and weaknesses internally, and opportunities and threats externally, decision-makers can develop a comprehensive view of their situation, leading to more informed strategies.\n",
      "\n",
      "2. **Cost-Benefit Analysis (CBA)**:\n",
      "   - **Description**: This method involves comparing the costs and benefits of different options to determine the most advantageous choice.\n",
      "   - **Support**: CBA quantifies potential outcomes, helping decision-makers visualize the trade-offs and prioritize options based on financial impact, promoting more rational financial decisions.\n",
      "\n",
      "3. **Decision Trees**:\n",
      "   - **Description**: A visual representation of possible options, outcomes, and their probabilities.\n",
      "   - **Support**: This framework allows for a clear mapping of available choices and their consequences, facilitating straightforward comparisons and enhancing clarity in complex decisions.\n",
      "\n",
      "4. **The Pareto Principle (80/20 Rule)**:\n",
      "   - **Description**: This principle posits that 80% of consequences come from 20% of causes.\n",
      "   - **Support**: By focusing on the most impactful factors, decision-makers can allocate resources and attention more effectively, ensuring that efforts are concentrated where they will yield the greatest benefit.\n",
      "\n",
      "5. **Multi-Criteria Decision Analysis (MCDA)**:\n",
      "   - **Description**: This process evaluates multiple conflicting criteria in decision-making.\n",
      "   - **Support**: MCDA helps to balance qualitative and quantitative factors, allowing for a more holistic view of options in scenarios where decisions must be made considering various stakeholder needs. \n",
      "\n",
      "6. **The Eisenhower Matrix**:\n",
      "   - **Description**: A simple grid that categorizes tasks based on urgency and importance.\n",
      "   - **Support**: This framework aids in prioritization by helping individuals focus on what truly matters, enhancing productivity and strategic focus.\n",
      "\n",
      "7. **OODA Loop (Observe, Orient, Decide, Act)**:\n",
      "   - **Description**: Developed by military strategist John Boyd, this cyclical process assists in decision-making in dynamic environments.\n",
      "   - **Support**: By emphasizing observation and orientation before acting, the OODA Loop promotes adaptability and quick responses to changing circumstances.\n",
      "\n",
      "8. **Six Thinking Hats**:\n",
      "   - **Description**: This framework involves looking at problems from six different perspectives, represented by colored hats.\n",
      "   - **Support**: By encouraging diverse thinking styles (e.g., emotional, analytical, creative, etc.), this approach helps teams reach more balanced and comprehensive decisions.\n",
      "\n",
      "9. **Risk Analysis**:\n",
      "   - **Description**: Involves identifying, assessing, and managing risks associated with a decision.\n",
      "   - **Support**: By evaluating potential risks and their impacts, decision-makers can develop contingency plans, leading to more robust and less risky choices.\n",
      "\n",
      "10. **Delphi Technique**:\n",
      "    - **Description**: A structured communication method that gathers input from a panel of experts through multiple rounds of Anonymous surveys.\n",
      "    - **Support**: This method helps mitigate bias and fosters consensus, improving the reliability of information on which decisions are based.\n",
      "\n",
      "Each of these frameworks provides unique advantages and methodologies suitable for different contexts, enhancing the analysis and reasoning behind decisions. By applying these frameworks, individuals and organizations can make more informed, strategic, and effective choices."
     ]
    }
   ],
   "source": [
    "await foreach (var streamChunk in semanticKernel.InvokePromptStreamingAsync(simplePrompt))\n",
    "{\n",
    "   Console.Write(streamChunk);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Execute a Semantic Kernel Prompt with a Custom Prompt Execution Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Kernel supports the configuration of prompt execution. The typical OpenAI and Azure OpenAI settings are exposed as configuration options that provide a different prompt experience. Try changing the MaxTokens, Tempature or other settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Execute a Semantic Kernel Prompt with a system prompt (Agent Persona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "ename": "Error",
     "evalue": "(2,41): error CS0246: The type or namespace name 'OpenAIPromptExecutionSettings' could not be found (are you missing a using directive or an assembly reference?)\r\n(10,1): error CS0246: The type or namespace name 'KernelArguments' could not be found (are you missing a using directive or an assembly reference?)\r\n(10,39): error CS0246: The type or namespace name 'KernelArguments' could not be found (are you missing a using directive or an assembly reference?)\r\n(11,35): error CS0103: The name 'semanticKernel' does not exist in the current context\r\n(11,77): error CS0103: The name 'simplePrompt' does not exist in the current context",
     "output_type": "error",
     "traceback": [
      "(2,41): error CS0246: The type or namespace name 'OpenAIPromptExecutionSettings' could not be found (are you missing a using directive or an assembly reference?)\r\n",
      "(10,1): error CS0246: The type or namespace name 'KernelArguments' could not be found (are you missing a using directive or an assembly reference?)\r\n",
      "(10,39): error CS0246: The type or namespace name 'KernelArguments' could not be found (are you missing a using directive or an assembly reference?)\r\n",
      "(11,35): error CS0103: The name 'semanticKernel' does not exist in the current context\r\n",
      "(11,77): error CS0103: The name 'simplePrompt' does not exist in the current context"
     ]
    }
   ],
   "source": [
    "// Create a new OpenAI prompt execution settings object\n",
    "var openAIPromptExecutionSettings = new OpenAIPromptExecutionSettings { \n",
    "    MaxTokens = 750, \n",
    "    Temperature = 0.0, \n",
    "    TopP = 1.0, \n",
    "    FrequencyPenalty = 0.0, \n",
    "    PresencePenalty = 0.0\n",
    "    };\n",
    "\n",
    "KernelArguments kernelArguments = new KernelArguments(openAIPromptExecutionSettings);\n",
    "await foreach (var streamChunk in semanticKernel.InvokePromptStreamingAsync(simplePrompt, kernelArguments))\n",
    "{\n",
    "   Console.Write(streamChunk);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the best practices of prompt engineering apply when using Semantic Kernel.  \n",
    "* Make the prompt more specific\n",
    "* Add structure to the output with formatting\n",
    "* Provide examples with few-shot prompting\n",
    "* Tell the AI what to do to avoid doing something wrong\n",
    "* Provide context to the AI\n",
    "* Using message roles in chat completion prompts\n",
    "* Give your AI words of encouragement  \n",
    "\n",
    "One best practice is to provide a common behaviour across all the LLM interactions in a system prompt. This system prompt is passed in on every single call the LLM with Semantic Kernel. These prompts can be dynamically created with a programming language like C#.\n",
    "\n",
    "Execute the cell below with a dynamic prompt template. Notice the different behavior of the output for decision frameworks. The decision framework responses are much more robust with decision intelligence information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "ename": "Error",
     "evalue": "(18,41): error CS0246: The type or namespace name 'OpenAIPromptExecutionSettings' could not be found (are you missing a using directive or an assembly reference?)\r\n(26,1): error CS0246: The type or namespace name 'KernelArguments' could not be found (are you missing a using directive or an assembly reference?)\r\n(26,39): error CS0246: The type or namespace name 'KernelArguments' could not be found (are you missing a using directive or an assembly reference?)\r\n(36,35): error CS0103: The name 'semanticKernel' does not exist in the current context",
     "output_type": "error",
     "traceback": [
      "(18,41): error CS0246: The type or namespace name 'OpenAIPromptExecutionSettings' could not be found (are you missing a using directive or an assembly reference?)\r\n",
      "(26,1): error CS0246: The type or namespace name 'KernelArguments' could not be found (are you missing a using directive or an assembly reference?)\r\n",
      "(26,39): error CS0246: The type or namespace name 'KernelArguments' could not be found (are you missing a using directive or an assembly reference?)\r\n",
      "(36,35): error CS0103: The name 'semanticKernel' does not exist in the current context"
     ]
    }
   ],
   "source": [
    "// Create a system prompt instruction to override the default system prompt\n",
    "// Add the System Prompt (Persona) to behave like a decision intelligence assistant\n",
    "var systemPromptForDecisions = \"\"\"\n",
    "You are a decision intelligence assistant. \n",
    "Assist the user in exploring options, reasoning through decisions, problem-solving, and applying systems thinking to various scenarios. \n",
    "Provide structured, logical, and comprehensive advice.\n",
    "\"\"\";\n",
    "var simplePrompt = \"Provide some decision frameworks that can help improve the quality of decisions.\";\n",
    "\n",
    "// How the prompt looks like to the LLM\n",
    "var simpleDecisionPromptTemplate = $\"\"\"\n",
    "System Prompt: {systemPromptForDecisions}\n",
    "\n",
    "Request from the user: {simplePrompt}\n",
    "\"\"\";\n",
    "\n",
    "// Create a new OpenAI prompt execution settings object\n",
    "var openAIPromptExecutionSettings = new OpenAIPromptExecutionSettings { \n",
    "    ChatSystemPrompt = systemPromptForDecisions,\n",
    "    MaxTokens = 750, \n",
    "    Temperature = 0.1, \n",
    "    TopP = 1.0, \n",
    "    FrequencyPenalty = 0.0, \n",
    "    PresencePenalty = 0.0\n",
    "    };\n",
    "KernelArguments kernelArguments = new KernelArguments(openAIPromptExecutionSettings);\n",
    "\n",
    "Console.WriteLine(\"Prompt To Send to the Azure OpenAI Chat Completion Service...\");\n",
    "Console.WriteLine(simpleDecisionPromptTemplate);\n",
    "Console.WriteLine(string.Empty);\n",
    "Console.WriteLine(\"----------------------------------------\");\n",
    "Console.WriteLine(string.Empty);\n",
    "Console.WriteLine(\"Response from the Azure OpenAI Chat Completion Service...\");\n",
    "\n",
    "\n",
    "await foreach (var streamChunk in semanticKernel.InvokePromptStreamingAsync(simplePrompt, kernelArguments))\n",
    "{\n",
    "   Console.Write(streamChunk);\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "csharp"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
