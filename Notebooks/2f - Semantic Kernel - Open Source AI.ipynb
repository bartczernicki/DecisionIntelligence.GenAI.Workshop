{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;padding-right: 10px\" width =\"40px\" src=\"https://raw.githubusercontent.com/bartczernicki/DecisionIntelligence.GenAI.Workshop/main/Images/SemanticKernelLogo.png\">\n",
    "\n",
    "## Semantic Kernel - Open Source AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Intelligence applied in this module:  \n",
    "* Listing key factors to consider when making a sound decision  \n",
    "* Decision Scenario: Use a decision framework (Ben Franklin's Pro & Con List) to create a decision plan  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recommended enterprise pattern is to scale Articial Intelligence strategy with three key areas:\n",
    "* Commercial AI (OpenAI and other proprietary AI providers)\n",
    "* Open-Source AI (open-source AI providers)\n",
    "* Partner (Vendor) AI (i.e. company HR Software, contract software)  \n",
    "\n",
    "These three areas together strategically form AI capability and capacity in what I like to call the \"AI Brain\". This is illustrated below.\n",
    "\n",
    "<img width =\"750px\" src=\"https://raw.githubusercontent.com/bartczernicki/DecisionIntelligence.GenAI.Workshop/main/Images/AIBrainPillars.png\">\n",
    "\n",
    "Semantic Kernel embraces AI orchestration across all the pillars mentioned above. It allows all types of models (commercial or proprietary) and almost any APIs to be orchestrated to faciliate better decision intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Get Started with LMStudio and Microsoft Phi-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module highlights how to use local AI models (i.e. LLMs) with Semantic Kernel. To illustrate this the Phi-3 model is run locally using LMStudio.\n",
    "\n",
    "Steps to get started:\n",
    "* Download & install LMStudio: https://lmstudio.ai/ (Windows, Mac or Linux) \n",
    "* Run the LMStudio studio application \n",
    "* In the LMStudio application, download the Phi-3.5-Mini GGUF file. LMStudio will inspect your hardware and let you know which quantized version of the model(s) is optimal for your hardware. Note: Even computers with small graphics cards can run these models well locally. Furthermore, laptops such as the Macbook Pro with Neural Engine can run LMStudio local models as well. \n",
    "* Start the LMStudio Server with the Phi-3.5-Mini model loaded. This will start a local REST endpoint with a URI similar to http://localhost:1234/v1 \n",
    "* The LMStudio local server does not have default security, you can simply check by navigating to this link in any browser to check if a model is loaded: http://localhost:1234/v1/models \n",
    "\n",
    "<img width =\"900px\" src=\"https://raw.githubusercontent.com/bartczernicki/DecisionIntelligence.GenAI.Workshop/main/Images/LMStudioServer.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Initialize Configuration Builder & Build the Semantic Kernel Orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the next cell to:\n",
    "* Use the Configuration Builder to use the local LMStudio Server  \n",
    "* Use the API configuration to build the Semantic Kernel orchestrator  \n",
    "* Notice there is no security being passed in and it is simply a URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Microsoft.SemanticKernel, 1.20.0</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#r \"nuget: Microsoft.SemanticKernel, 1.20.0\"\n",
    "\n",
    "using Microsoft.SemanticKernel;\n",
    "using Microsoft.SemanticKernel.ChatCompletion;\n",
    "using Microsoft.SemanticKernel.Connectors.OpenAI;\n",
    "\n",
    "#pragma warning disable SKEXP0010 \n",
    "var semanticKernel = Kernel.CreateBuilder()\n",
    "    .AddOpenAIChatCompletion(\n",
    "        modelId: \"lmstudio-community/Phi-3.5-mini-instruct-GGUF\",\n",
    "        endpoint: new Uri(\"http://localhost:1234/v1/\"),\n",
    "        apiKey: null)\n",
    "    .Build();\n",
    "#pragma warning restore SKEXP0010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Use Semantic Kernel with Open Source AI Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Kernel allows one to interact with any API service that adheres to the OpenAI specification. Notice the method to add LMStudio capability was simply enabled via the **AddOpenAIChatCompletion** method.\n",
    "\n",
    "Execute the cell below about decision factors for a investment property. Note:\n",
    "* OpenAI Prompt Execution Settings are the same in LMStudio as they are for OpenAI and Azure OpenAI\n",
    "* Passing in arguents works the same way in Semantic Kernel\n",
    "* Streaming works as well in Semantic Kernel for supported services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"Don't wait to buy real estate. Buy real estate and wait.\"  \n",
    ">\n",
    "> -- <cite>Will Rodgers (American humorist, actor, and social commentator in the early 20th century)</cite>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "ename": "Error",
     "evalue": "System.ClientModel.ClientResultException: No connection could be made because the target machine actively refused it. (localhost:1234)\r\n ---> System.Net.Http.HttpRequestException: No connection could be made because the target machine actively refused it. (localhost:1234)\r\n ---> System.Net.Sockets.SocketException (10061): No connection could be made because the target machine actively refused it.\r\n   at System.Net.Sockets.Socket.AwaitableSocketAsyncEventArgs.ThrowException(SocketError error, CancellationToken cancellationToken)\r\n   at System.Net.Sockets.Socket.AwaitableSocketAsyncEventArgs.System.Threading.Tasks.Sources.IValueTaskSource.GetResult(Int16 token)\r\n   at System.Net.Sockets.Socket.<ConnectAsync>g__WaitForConnectWithCancellation|285_0(AwaitableSocketAsyncEventArgs saea, ValueTask connectTask, CancellationToken cancellationToken)\r\n   at System.Net.Http.HttpConnectionPool.ConnectToTcpHostAsync(String host, Int32 port, HttpRequestMessage initialRequest, Boolean async, CancellationToken cancellationToken)\r\n   --- End of inner exception stack trace ---\r\n   at System.Net.Http.HttpConnectionPool.ConnectToTcpHostAsync(String host, Int32 port, HttpRequestMessage initialRequest, Boolean async, CancellationToken cancellationToken)\r\n   at System.Net.Http.HttpConnectionPool.ConnectAsync(HttpRequestMessage request, Boolean async, CancellationToken cancellationToken)\r\n   at System.Net.Http.HttpConnectionPool.CreateHttp11ConnectionAsync(HttpRequestMessage request, Boolean async, CancellationToken cancellationToken)\r\n   at System.Net.Http.HttpConnectionPool.AddHttp11ConnectionAsync(QueueItem queueItem)\r\n   at System.Threading.Tasks.TaskCompletionSourceWithCancellation`1.WaitWithCancellationAsync(CancellationToken cancellationToken)\r\n   at System.Net.Http.HttpConnectionPool.SendWithVersionDetectionAndRetryAsync(HttpRequestMessage request, Boolean async, Boolean doRequestAuth, CancellationToken cancellationToken)\r\n   at System.Net.Http.DiagnosticsHandler.SendAsyncCore(HttpRequestMessage request, Boolean async, CancellationToken cancellationToken)\r\n   at System.Net.Http.RedirectHandler.SendAsync(HttpRequestMessage request, Boolean async, CancellationToken cancellationToken)\r\n   at System.Net.Http.HttpClient.<SendAsync>g__Core|83_0(HttpRequestMessage request, HttpCompletionOption completionOption, CancellationTokenSource cts, Boolean disposeCts, CancellationTokenSource pendingRequestsCts, CancellationToken originalCancellationToken)\r\n   at System.ClientModel.Primitives.HttpClientPipelineTransport.ProcessSyncOrAsync(PipelineMessage message, Boolean async)\r\n   --- End of inner exception stack trace ---\r\n   at System.ClientModel.Primitives.HttpClientPipelineTransport.ProcessSyncOrAsync(PipelineMessage message, Boolean async)\r\n   at System.ClientModel.Primitives.HttpClientPipelineTransport.ProcessCoreAsync(PipelineMessage message)\r\n   at System.ClientModel.Primitives.PipelineTransport.ProcessSyncOrAsync(PipelineMessage message, Boolean async)\r\n   at System.ClientModel.Primitives.PipelineTransport.ProcessAsync(PipelineMessage message)\r\n   at System.ClientModel.Primitives.PipelineTransport.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n   at System.ClientModel.Primitives.PipelinePolicy.ProcessNextAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n   at System.ClientModel.Primitives.ApiKeyAuthenticationPolicy.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n   at System.ClientModel.Primitives.PipelinePolicy.ProcessNextAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n   at System.ClientModel.Primitives.ClientRetryPolicy.ProcessSyncOrAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex, Boolean async)\r\n   at System.ClientModel.Primitives.ClientRetryPolicy.ProcessSyncOrAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex, Boolean async)\r\n   at System.ClientModel.Primitives.ClientRetryPolicy.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n   at GenericActionPipelinePolicy.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n   at OpenAI.GenericActionPipelinePolicy.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n   at OpenAI.GenericActionPipelinePolicy.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n   at System.ClientModel.Primitives.ClientPipeline.SendAsync(PipelineMessage message)\r\n   at OpenAI.ClientPipelineExtensions.ProcessMessageAsync(ClientPipeline pipeline, PipelineMessage message, RequestOptions options)\r\n   at OpenAI.Chat.ChatClient.CompleteChatAsync(BinaryContent content, RequestOptions options)\r\n   at OpenAI.Chat.ChatClient.<>c__DisplayClass9_0.<<CompleteChatStreamingAsync>g__getResultAsync|0>d.MoveNext()\r\n--- End of stack trace from previous location ---\r\n   at OpenAI.Chat.AsyncStreamingChatCompletionUpdateCollection.AsyncStreamingChatUpdateEnumerator.CreateEventEnumeratorAsync()\r\n   at OpenAI.Chat.AsyncStreamingChatCompletionUpdateCollection.AsyncStreamingChatUpdateEnumerator.System.Collections.Generic.IAsyncEnumerator<OpenAI.Chat.StreamingChatCompletionUpdate>.MoveNextAsync()\r\n   at Microsoft.SemanticKernel.Connectors.OpenAI.ClientCore.GetStreamingChatMessageContentsAsync(String targetModel, ChatHistory chatHistory, PromptExecutionSettings executionSettings, Kernel kernel, CancellationToken cancellationToken)+MoveNext()\r\n   at Microsoft.SemanticKernel.Connectors.OpenAI.ClientCore.GetStreamingChatMessageContentsAsync(String targetModel, ChatHistory chatHistory, PromptExecutionSettings executionSettings, Kernel kernel, CancellationToken cancellationToken)+MoveNext()\r\n   at Microsoft.SemanticKernel.Connectors.OpenAI.ClientCore.GetStreamingChatMessageContentsAsync(String targetModel, ChatHistory chatHistory, PromptExecutionSettings executionSettings, Kernel kernel, CancellationToken cancellationToken)+System.Threading.Tasks.Sources.IValueTaskSource<System.Boolean>.GetResult()\r\n   at Microsoft.SemanticKernel.KernelFunctionFromPrompt.InvokeStreamingCoreAsync[TResult](Kernel kernel, KernelArguments arguments, CancellationToken cancellationToken)+MoveNext()\r\n   at Microsoft.SemanticKernel.KernelFunctionFromPrompt.InvokeStreamingCoreAsync[TResult](Kernel kernel, KernelArguments arguments, CancellationToken cancellationToken)+MoveNext()\r\n   at Microsoft.SemanticKernel.KernelFunctionFromPrompt.InvokeStreamingCoreAsync[TResult](Kernel kernel, KernelArguments arguments, CancellationToken cancellationToken)+System.Threading.Tasks.Sources.IValueTaskSource<System.Boolean>.GetResult()\r\n   at Microsoft.SemanticKernel.KernelFunction.InvokeStreamingAsync[TResult](Kernel kernel, KernelArguments arguments, CancellationToken cancellationToken)+MoveNext()\r\n   at Microsoft.SemanticKernel.KernelFunction.InvokeStreamingAsync[TResult](Kernel kernel, KernelArguments arguments, CancellationToken cancellationToken)+MoveNext()\r\n   at Microsoft.SemanticKernel.KernelFunction.InvokeStreamingAsync[TResult](Kernel kernel, KernelArguments arguments, CancellationToken cancellationToken)+System.Threading.Tasks.Sources.IValueTaskSource<System.Boolean>.GetResult()\r\n   at Submission#9.<<Initialize>>d__0.MoveNext()\r\n--- End of stack trace from previous location ---\r\n   at Submission#9.<<Initialize>>d__0.MoveNext()\r\n--- End of stack trace from previous location ---\r\n   at Microsoft.CodeAnalysis.Scripting.ScriptExecutionState.RunSubmissionsAsync[TResult](ImmutableArray`1 precedingExecutors, Func`2 currentExecutor, StrongBox`1 exceptionHolderOpt, Func`2 catchExceptionOpt, CancellationToken cancellationToken)",
     "output_type": "error",
     "traceback": [
      "System.ClientModel.ClientResultException: No connection could be made because the target machine actively refused it. (localhost:1234)\r\n",
      " ---> System.Net.Http.HttpRequestException: No connection could be made because the target machine actively refused it. (localhost:1234)\r\n",
      " ---> System.Net.Sockets.SocketException (10061): No connection could be made because the target machine actively refused it.\r\n",
      "   at System.Net.Sockets.Socket.AwaitableSocketAsyncEventArgs.ThrowException(SocketError error, CancellationToken cancellationToken)\r\n",
      "   at System.Net.Sockets.Socket.AwaitableSocketAsyncEventArgs.System.Threading.Tasks.Sources.IValueTaskSource.GetResult(Int16 token)\r\n",
      "   at System.Net.Sockets.Socket.<ConnectAsync>g__WaitForConnectWithCancellation|285_0(AwaitableSocketAsyncEventArgs saea, ValueTask connectTask, CancellationToken cancellationToken)\r\n",
      "   at System.Net.Http.HttpConnectionPool.ConnectToTcpHostAsync(String host, Int32 port, HttpRequestMessage initialRequest, Boolean async, CancellationToken cancellationToken)\r\n",
      "   --- End of inner exception stack trace ---\r\n",
      "   at System.Net.Http.HttpConnectionPool.ConnectToTcpHostAsync(String host, Int32 port, HttpRequestMessage initialRequest, Boolean async, CancellationToken cancellationToken)\r\n",
      "   at System.Net.Http.HttpConnectionPool.ConnectAsync(HttpRequestMessage request, Boolean async, CancellationToken cancellationToken)\r\n",
      "   at System.Net.Http.HttpConnectionPool.CreateHttp11ConnectionAsync(HttpRequestMessage request, Boolean async, CancellationToken cancellationToken)\r\n",
      "   at System.Net.Http.HttpConnectionPool.AddHttp11ConnectionAsync(QueueItem queueItem)\r\n",
      "   at System.Threading.Tasks.TaskCompletionSourceWithCancellation`1.WaitWithCancellationAsync(CancellationToken cancellationToken)\r\n",
      "   at System.Net.Http.HttpConnectionPool.SendWithVersionDetectionAndRetryAsync(HttpRequestMessage request, Boolean async, Boolean doRequestAuth, CancellationToken cancellationToken)\r\n",
      "   at System.Net.Http.DiagnosticsHandler.SendAsyncCore(HttpRequestMessage request, Boolean async, CancellationToken cancellationToken)\r\n",
      "   at System.Net.Http.RedirectHandler.SendAsync(HttpRequestMessage request, Boolean async, CancellationToken cancellationToken)\r\n",
      "   at System.Net.Http.HttpClient.<SendAsync>g__Core|83_0(HttpRequestMessage request, HttpCompletionOption completionOption, CancellationTokenSource cts, Boolean disposeCts, CancellationTokenSource pendingRequestsCts, CancellationToken originalCancellationToken)\r\n",
      "   at System.ClientModel.Primitives.HttpClientPipelineTransport.ProcessSyncOrAsync(PipelineMessage message, Boolean async)\r\n",
      "   --- End of inner exception stack trace ---\r\n",
      "   at System.ClientModel.Primitives.HttpClientPipelineTransport.ProcessSyncOrAsync(PipelineMessage message, Boolean async)\r\n",
      "   at System.ClientModel.Primitives.HttpClientPipelineTransport.ProcessCoreAsync(PipelineMessage message)\r\n",
      "   at System.ClientModel.Primitives.PipelineTransport.ProcessSyncOrAsync(PipelineMessage message, Boolean async)\r\n",
      "   at System.ClientModel.Primitives.PipelineTransport.ProcessAsync(PipelineMessage message)\r\n",
      "   at System.ClientModel.Primitives.PipelineTransport.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n",
      "   at System.ClientModel.Primitives.PipelinePolicy.ProcessNextAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n",
      "   at System.ClientModel.Primitives.ApiKeyAuthenticationPolicy.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n",
      "   at System.ClientModel.Primitives.PipelinePolicy.ProcessNextAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n",
      "   at System.ClientModel.Primitives.ClientRetryPolicy.ProcessSyncOrAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex, Boolean async)\r\n",
      "   at System.ClientModel.Primitives.ClientRetryPolicy.ProcessSyncOrAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex, Boolean async)\r\n",
      "   at System.ClientModel.Primitives.ClientRetryPolicy.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n",
      "   at GenericActionPipelinePolicy.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n",
      "   at OpenAI.GenericActionPipelinePolicy.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n",
      "   at OpenAI.GenericActionPipelinePolicy.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)\r\n",
      "   at System.ClientModel.Primitives.ClientPipeline.SendAsync(PipelineMessage message)\r\n",
      "   at OpenAI.ClientPipelineExtensions.ProcessMessageAsync(ClientPipeline pipeline, PipelineMessage message, RequestOptions options)\r\n",
      "   at OpenAI.Chat.ChatClient.CompleteChatAsync(BinaryContent content, RequestOptions options)\r\n",
      "   at OpenAI.Chat.ChatClient.<>c__DisplayClass9_0.<<CompleteChatStreamingAsync>g__getResultAsync|0>d.MoveNext()\r\n",
      "--- End of stack trace from previous location ---\r\n",
      "   at OpenAI.Chat.AsyncStreamingChatCompletionUpdateCollection.AsyncStreamingChatUpdateEnumerator.CreateEventEnumeratorAsync()\r\n",
      "   at OpenAI.Chat.AsyncStreamingChatCompletionUpdateCollection.AsyncStreamingChatUpdateEnumerator.System.Collections.Generic.IAsyncEnumerator<OpenAI.Chat.StreamingChatCompletionUpdate>.MoveNextAsync()\r\n",
      "   at Microsoft.SemanticKernel.Connectors.OpenAI.ClientCore.GetStreamingChatMessageContentsAsync(String targetModel, ChatHistory chatHistory, PromptExecutionSettings executionSettings, Kernel kernel, CancellationToken cancellationToken)+MoveNext()\r\n",
      "   at Microsoft.SemanticKernel.Connectors.OpenAI.ClientCore.GetStreamingChatMessageContentsAsync(String targetModel, ChatHistory chatHistory, PromptExecutionSettings executionSettings, Kernel kernel, CancellationToken cancellationToken)+MoveNext()\r\n",
      "   at Microsoft.SemanticKernel.Connectors.OpenAI.ClientCore.GetStreamingChatMessageContentsAsync(String targetModel, ChatHistory chatHistory, PromptExecutionSettings executionSettings, Kernel kernel, CancellationToken cancellationToken)+System.Threading.Tasks.Sources.IValueTaskSource<System.Boolean>.GetResult()\r\n",
      "   at Microsoft.SemanticKernel.KernelFunctionFromPrompt.InvokeStreamingCoreAsync[TResult](Kernel kernel, KernelArguments arguments, CancellationToken cancellationToken)+MoveNext()\r\n",
      "   at Microsoft.SemanticKernel.KernelFunctionFromPrompt.InvokeStreamingCoreAsync[TResult](Kernel kernel, KernelArguments arguments, CancellationToken cancellationToken)+MoveNext()\r\n",
      "   at Microsoft.SemanticKernel.KernelFunctionFromPrompt.InvokeStreamingCoreAsync[TResult](Kernel kernel, KernelArguments arguments, CancellationToken cancellationToken)+System.Threading.Tasks.Sources.IValueTaskSource<System.Boolean>.GetResult()\r\n",
      "   at Microsoft.SemanticKernel.KernelFunction.InvokeStreamingAsync[TResult](Kernel kernel, KernelArguments arguments, CancellationToken cancellationToken)+MoveNext()\r\n",
      "   at Microsoft.SemanticKernel.KernelFunction.InvokeStreamingAsync[TResult](Kernel kernel, KernelArguments arguments, CancellationToken cancellationToken)+MoveNext()\r\n",
      "   at Microsoft.SemanticKernel.KernelFunction.InvokeStreamingAsync[TResult](Kernel kernel, KernelArguments arguments, CancellationToken cancellationToken)+System.Threading.Tasks.Sources.IValueTaskSource<System.Boolean>.GetResult()\r\n",
      "   at Submission#9.<<Initialize>>d__0.MoveNext()\r\n",
      "--- End of stack trace from previous location ---\r\n",
      "   at Submission#9.<<Initialize>>d__0.MoveNext()\r\n",
      "--- End of stack trace from previous location ---\r\n",
      "   at Microsoft.CodeAnalysis.Scripting.ScriptExecutionState.RunSubmissionsAsync[TResult](ImmutableArray`1 precedingExecutors, Func`2 currentExecutor, StrongBox`1 exceptionHolderOpt, Func`2 catchExceptionOpt, CancellationToken cancellationToken)"
     ]
    }
   ],
   "source": [
    "// Prompting works in a very similar way to the OpenAI API\n",
    "\n",
    "// Create decision intelligence prompt on the topic of purchasing a secondary home as an investment property\n",
    "// Provide detailed decision-making criteria for evaluating the investment decision\n",
    "var simplePrompt = \"\"\"\n",
    "You are considering purchasing a secondary home as an investment property. \n",
    "\n",
    "What key factors should you evaluate to ensure a sound investment decision, including financial, market, and property-specific considerations? \n",
    "Outline the critical steps and criteria for assessing location, potential rental income, financing options, long-term property value, and associated risks.\n",
    "\"\"\";\n",
    "\n",
    "// You can set the typical OpenAI settings with most open-source models\n",
    "var openAIPromptExecutionSettings = new OpenAIPromptExecutionSettings { \n",
    "    MaxTokens = 750, \n",
    "    Temperature = 0.1, \n",
    "    TopP = 1.0, \n",
    "    FrequencyPenalty = 0.0, \n",
    "    PresencePenalty = 0.0\n",
    "    };\n",
    "KernelArguments kernelArguments = new KernelArguments(openAIPromptExecutionSettings);\n",
    "\n",
    "// Most open-source GenAI moodels support streaming as well\n",
    "await foreach (var streamChunk in semanticKernel.InvokePromptStreamingAsync(simplePrompt, kernelArguments))\n",
    "{\n",
    "   Console.Write(streamChunk);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Using the Ben Franklin Decision Framework to Make Great Decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"By failing to prepare, you are preparing to fail.\" \n",
    ">\n",
    "> -- <cite>Ben Franklin (Founding Father of the United States, inventor, writer)</cite> \n",
    "\n",
    "<img width =\"750px\" src=\"https://raw.githubusercontent.com/bartczernicki/Articles/main/20230326-Make-Great-Decisions-Using-Ben-Franklins-Pros-And-Cons-Method/Image-BenFranklinDecisionMakingMethod.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tom Brady's use of a Decision Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tom Brady's decision to join the Tampa Bay Buccaneers in 2020 marked a significant in his legendary NFL career. After 20 seasons and six Super Bowl championships with the New England Patriots, Brady became a free agent and chose to sign with the Bucs. How did he arrive at this decision? On the Fox broadcast on 09.29.2024, while covering the Buccaneers vs Philadelphia Eagles game, Tom Brady described how he arrived at this decision.\n",
    "\n",
    "In the screenshot below, Tom Brady is holding up some small paper cards he is showing the audience of the broadcast. Brady mentioned he wrote down the personal decision criteria that was important and how each team compared in that criteria (salary, weather etc). He used this to select the Tampa Bay Buccaneers as his team, where he went on to win a Super Bowl in his first year there! **Tom Brady used the \"Ben Franklin Decision Framework\", 250 years after it's inception to decide where to play NFL quaterback!**  \n",
    "\n",
    "<img width =\"750px\" src=\"https://raw.githubusercontent.com/bartczernicki/DecisionIntelligence.GenAI.Workshop/main/Images/BenFranklinDecisionFramework-TomBrady.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps for Ben Franklin's Decision Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the steps Ben Franklin recommends when making a decision, which he called his \"Decision Making Method of Moral Algebra\":  \n",
    "- Frame a decision that has two options (Yes or a No)\n",
    "- Divide an area into two competing halves: a \"Pro\" side and \"Con\" side\n",
    "- Label the top of one side \"Pro\" (for) and the other \"Con\" (against)\n",
    "- Under each respective side, over a period of time (Ben Franklin recommended days, this could be minutes) write down various reasons/arguments that support (Pro) or are against (Con) the decision\n",
    "- After spending some time thinking exhaustively and writing down the reasons, weight the different Pro and Con reasons/arguments\n",
    "- Determine the relative importance of each reason or argument. This is done by taking reasons/arguments that are of similar value (weight) and crossing them off of the other competing half. Multiple reasons can be combined from one side to form a \"subjective\" value (weight) to balance out the other half. (For example, two medium \"Pro\" reasons might add up to an equal value of a single important \"Con\" reason)\n",
    "- The side with the most remaining reasons is the option one should select for the decision in question\n",
    "\n",
    "Learn more about Ben Franklin's Decision Framework: https://medium.com/@bartczernicki/make-great-decisions-using-ben-franklins-decision-making-method-c7fb8b17905c  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "ename": "Error",
     "evalue": "(17,35): error CS0103: The name 'semanticKernel' does not exist in the current context\r\n(17,107): error CS0103: The name 'kernelArguments' does not exist in the current context",
     "output_type": "error",
     "traceback": [
      "(17,35): error CS0103: The name 'semanticKernel' does not exist in the current context\r\n",
      "(17,107): error CS0103: The name 'kernelArguments' does not exist in the current context"
     ]
    }
   ],
   "source": [
    "// You can also use a template to provide more context to the model combining system and user prompts\n",
    "var systemPrompt = \"\"\"\n",
    "You are a decision intelligence assistant. \n",
    "Your role is to guide users in exploring options, analyzing decisions, solving complex problems, and applying systems thinking to diverse scenarios. \n",
    "Provide structured, logical, and comprehensive advice, ensuring clarity, depth, and actionable insights to support informed decision-making.\n",
    "\"\"\";\n",
    "var simpleDecisionPrompt = \"\"\"\n",
    "Apply the Ben Franklin Decision-Making Framework to evaluate whether or not to take a luxury family vacation. \n",
    "\"\"\";\n",
    "\n",
    "var simpleDecisionPromptTemplate = $\"\"\"\n",
    "{systemPrompt}\n",
    "--------------\n",
    "{simpleDecisionPrompt}\n",
    "\"\"\";\n",
    "\n",
    "await foreach (var streamChunk in semanticKernel.InvokePromptStreamingAsync(simpleDecisionPromptTemplate, kernelArguments))\n",
    "{\n",
    "   Console.Write(streamChunk);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Multiple Different Service Providers  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Kernel can include mutliple AI service providers. This allows for hybrid workflows from a single Semantic Kernel instance like: \n",
    "* Capability Optimization. For example, using SLMs for domain specific tasks and LLMs for complex decision reasoning\n",
    "* Capacity Optimization. Splitting functions, plugins, personas or agents across different AI services\n",
    "* Use AI Service selector dynamically allocate AI service execution resources  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Microsoft.Extensions.Configuration, 8.0.0</span></li><li><span>Microsoft.Extensions.Configuration.Json, 8.0.0</span></li><li><span>Microsoft.SemanticKernel, 1.20.0</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Import the required NuGet configuration packages\n",
    "#r \"nuget: Microsoft.Extensions.Configuration, 8.0.0\"\n",
    "#r \"nuget: Microsoft.Extensions.Configuration.Json, 8.0.0\"\n",
    "#r \"nuget: Microsoft.SemanticKernel, 1.20.0\"\n",
    "\n",
    "using Microsoft.Extensions.Configuration;\n",
    "using System.IO;\n",
    "using Microsoft.SemanticKernel;\n",
    "using Microsoft.SemanticKernel.ChatCompletion;\n",
    "using Microsoft.SemanticKernel.Connectors.OpenAI;\n",
    "\n",
    "// Load the configuration settings from the local.settings.json and secrets.settings.json files\n",
    "// The secrets.settings.json file is used to store sensitive information such as API keys\n",
    "var configurationBuilder = new ConfigurationBuilder()\n",
    "    .SetBasePath(Directory.GetCurrentDirectory())\n",
    "    .AddJsonFile(\"local.settings.json\", optional: true, reloadOnChange: true)\n",
    "    .AddJsonFile(\"secrets.settings.json\", optional: true, reloadOnChange: true);\n",
    "var config = configurationBuilder.Build();\n",
    "\n",
    "// Retrieve the configuration settings for the Azure OpenAI service\n",
    "var azureOpenAIEndpoint = config[\"AzureOpenAI:Endpoint\"];\n",
    "var azureOpenAIAPIKey = config[\"AzureOpenAI:APIKey\"];\n",
    "var azureOpenAIModelDeploymentName = config[\"AzureOpenAI:ModelDeploymentName\"];\n",
    "\n",
    "// Example to build a Kernel with Azure OpenAI and Opensource AI\n",
    "#pragma warning disable SKEXP0010 \n",
    "var semanticKernel = Kernel.CreateBuilder()\n",
    "    .AddAzureOpenAIChatCompletion(\n",
    "        deploymentName: azureOpenAIModelDeploymentName,\n",
    "        endpoint: azureOpenAIEndpoint,\n",
    "        apiKey: azureOpenAIAPIKey)\n",
    "    .AddOpenAIChatCompletion(\n",
    "        modelId: \"Phi-3\",\n",
    "        endpoint: new Uri(\"http://localhost:1234/v1/\"),\n",
    "        apiKey: \"LMStudio\")\n",
    "    .Build();\n",
    "#pragma warning restore SKEXP0010"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "csharp"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
