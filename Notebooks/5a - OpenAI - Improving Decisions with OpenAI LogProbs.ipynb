{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;padding-right: 10px; filter: invert(89%) sepia(6%) saturate(1622%) hue-rotate(161deg) brightness(97%) contrast(85%);\" width =\"40px\" src=\"https://raw.githubusercontent.com/bartczernicki/DecisionIntelligence.GenAI.Workshop/refs/heads/main/Images/openai-2.svg\">\n",
    "\n",
    "## OpenAI - Improving Decisions with OpenAI LogProbs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work in Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Azure OpenAI LogProbs Title](https://raw.githubusercontent.com/bartczernicki/AzureOpenAILogProbs/master/AzureOpenAILogProbs/Images/AzureLogProbs-Title.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Information on Log Probabilities  \n",
    "\n",
    "What are LogProbs (Log Probabilities)? Most current LLMs process prompt instructions by predicting the next token and iterate through each token until they reach a stopping point (i.e. max token length, completing the user instructions). For each token that is considered for output is processed through an internal LLM pipeline that outputs a statistical probability distribution of \"best match\" tokens to select from. Based on configurations (temperature, top_p etc.) these token probabilities can be calculated and then the LLM selects the next \"best match\" token based on the different configurations. Because these LLMs are probabilistic in nature, this is why you may see different tokens output for the same prompt instruction sent to the (LLM) model.  \n",
    "\n",
    "Below is an example of a Q&A scenario and the associated probabilities for the two tokens (words) that were selected to answer the question: **\"Who was the first president of the United States?\"**. In the example below the model answered with two tokens \"George\" \"Washington\", using the token probabilities of 99.62% and 99.99% respectively. Note that there were other tokens available for selection, but the LLM's inherent knowledge and reasoning capability (from being trained on volumunous amount of data) confidently increased the probability of these two tokens: \"George\" and \"Washington\".\n",
    "\n",
    "There are settings that can calibrate how strict or creative an LLM is. For example, you may have heard of an (LLM) model setting called **Temperature** that essentially increases the chance of lower probability tokens being selected.  \n",
    "\n",
    "![Azure LogProbs Example](https://raw.githubusercontent.com/bartczernicki/AzureOpenAILogProbs/master/AzureOpenAILogProbs/Images/AzureLogProbs-Example.png)\n",
    "\n",
    "Need more info? Recommended Reading on the background of Azure OpenAI LogProbs:  \n",
    "   * OpenAI Cookbook - LogProbs: https://cookbook.openai.com/examples/using_logprobs  \n",
    "   * What are LogProbs?: https://www.ignorance.ai/p/what-are-logprobs  \n",
    "\n",
    "## Using LogProbs for Improving GenAI Quality\n",
    "\n",
    "There are various proven and new improving techniques that use multiple calls to a model or several models to arrive at a response, conclusion or a quality decision. Currently, most ways LLMs are used in GenAI production systems is with grounding (RAG) by providing additional contextual information. The (LLM) model is instructed to answer a question, reason over that information etc. However, with poor grounding techniques, this can result in lower quality results.  \n",
    "\n",
    "Azure OpenAI LogProbs are an advanced technique that can help and be can utilized to gauge the confidence (probability) of the model's response.\n",
    "This tremendous capability can empower the GenAI system to self-correct or guide the user/agent to arrive at an improved quality response.\n",
    "\n",
    "The power of LogProbs is illustrated below with the diagram of the GenAI workflow. Notice that there are two paths (left and right):  \n",
    "* The left path is the traditional path most GenAI applications follow. You ask a question and receive a response from an LLM. This typical workflow on the left is what one will find in most current GenAI Chat applications.  \n",
    "* The right path is a **\"quality enhacement\"** to the workflow. In parallel, one can ask the LLM \"LLM, do you have enough information to answer this question and how sure are you there enough information?\"! Notice from the diagram below with this \"quality enhancement\" now includes:  \n",
    "    1) **Answer** to the question  \n",
    "    2) **Does the Model Have Enough Information to Answer the Question** - True or False estimate from the (LLM) model  \n",
    "    3) **Probability of Having Enough Information to Answer the Question** - Calculated from LogProbs; which can be used for additional statistical inference or decision threshholding  \n",
    "\n",
    "![Azure LogProbs Workflow](https://raw.githubusercontent.com/bartczernicki/AzureOpenAILogProbs/master/AzureOpenAILogProbs/Images/AzureLogProbs-LogProbsWorkflow.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/bartczernicki/AzureOpenAILogProbs "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "csharp"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
